{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization - 似乎叫standardization更好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization(mean 归一化): $$\\frac{x - \\bar{x}}{x_{max} - x_{min}} \\rightarrow [0,1]$$\n",
    "\n",
    "Rescaling(min-max 归一化): $$\\frac{x - x_{min}}{x_{max} - x_{min}} \\rightarrow [0,1]$$\n",
    "\n",
    "Standardization(标准化): $$\\frac{x - \\bar{x}}{\\sigma} \\rightarrow mean=0, std=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准化是指将数处理成均值为0，标准差为1的分布。标准化处理使得数据的各个特征分布相近，这往往能更容易的训练出有效的模型。\n",
    "\n",
    "对深层NN来说，即使输入的数据已经标准化了过，但是在训练中模型参数的更新依然很容易造成深隐层的输出的剧烈变化。这种计算数值的不稳定性通常让我们难以训练处有效的深度模型。\n",
    "\n",
    "Batch Normlization的提出正式为了应对深度模型训练的挑战，在模型训练的时候，BN利用mini-batch上的均值和标准差，不断调整神经网络中间输出，从而使得整个神经网络在各层中间输出的数值更稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对全连接层做BN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常，我们将批量归一化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为$\\boldsymbol{u}$，权重参数和偏差参数分别为$\\boldsymbol{W}$和$\\boldsymbol{b}$，激活函数为ϕ。设批量归一化的运算符为$\\boldsymbol{BN}$。那么，使用批量归一化的全连接层的输出为：\n",
    "$$\\phi(\\text{BN}(\\boldsymbol{x})),$$\n",
    "其中批量归一化输入由$\\boldsymbol{x}$仿射变换\n",
    "$$\\boldsymbol{x} = \\boldsymbol{W\\boldsymbol{u} + \\boldsymbol{b}}$$\n",
    "得到。考虑一个由m个样本组成的小批量，仿射变换的输出为一个新的小批量$\\mathcal{B} = \\{\\boldsymbol{x}^{(1)}, \\ldots, \\boldsymbol{x}^{(m)} \\}$。它们正是批量归一化层的输入。对于小批量$\\mathcal{B}$中任意样本$\\boldsymbol{x}^{(i)} \\in \\mathbb{R}^d, 1 \\leq i \\leq m$，批量归一化层的输出同样是$\\boldsymbol{d}$维向量\n",
    "$$\\boldsymbol{y}^{(i)} = \\text{BN}(\\boldsymbol{x}^{(i)}),$$\n",
    "并由以下几步求得。首先，对小批量$\\mathcal{B}$求均值和方差：\n",
    "$$\\boldsymbol{\\mu}_\\mathcal{B} \\leftarrow \\frac{1}{m}\\sum_{i = 1}^{m} \\boldsymbol{x}^{(i)},$$\n",
    "$$\\boldsymbol{\\sigma}_\\mathcal{B}^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}(\\boldsymbol{x}^{(i)} - \\boldsymbol{\\mu}_\\mathcal{B})^2,$$\n",
    "其中的平方计算是按元素求平方。接下来，使用按元素开方和按元素除法对$\\boldsymbol{x}^{(i)}$标准化：\n",
    "$$\\hat{\\boldsymbol{x}}^{(i)} \\leftarrow \\frac{\\boldsymbol{x}^{(i)} - \\boldsymbol{\\mu}_\\mathcal{B}}{\\sqrt{\\boldsymbol{\\sigma}_\\mathcal{B}^2 + \\epsilon}},$$\n",
    "这里$\\epsilon > 0$是一个很小的常数，保证分母大于0。\n",
    "\n",
    "在上面标准化的基础上，**批量归一化层引入了两个可以学习的模型参数**，拉伸（scale）参数$\\boldsymbol{\\gamma}$和偏移（shift）参数$\\boldsymbol{\\beta}$。这两个参数和$\\boldsymbol{x}^{(i)}$形状相同，皆为d维向量。它们与$\\hat{\\boldsymbol{x}}^{(i)}$分别做按元素乘法（符号$\\odot$）和加法计算：\n",
    "$${\\boldsymbol{y}}^{(i)} \\leftarrow \\boldsymbol{\\gamma} \\odot \\hat{\\boldsymbol{x}}^{(i)} + \\boldsymbol{\\beta}.$$\n",
    "至此，我们得到了$\\boldsymbol{x}^{(i)}$的批量归一化的输出$\\boldsymbol{y}^{(i)}$。 值得注意的是，可学习的拉伸和偏移参数保留了不对$\\boldsymbol{x}^{(i)}$做批量归一化的可能：此时只需学出$\\boldsymbol{\\gamma} = \\sqrt{\\boldsymbol{\\sigma}_\\mathcal{B}^2 + \\epsilon}$和$\\boldsymbol{\\beta} = \\boldsymbol{\\mu}_\\mathcal{B}$。我们可以对此这样理解：**如果批量归一化无益，理论上，学出的模型可以不使用批量归一化。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对卷积层做BN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道，我们需要对**这些通道的输出分别做批量归一化**，且每个通道都拥有独立的拉伸和偏移参数，并均为标量。设小批量中有 m个样本。在单个通道上，假设卷积计算输出的高和宽分别为 p和 q 。我们需要对该通道中 m×p×q个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中 m×p×q个元素的均值和方差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测时的BN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用批量归一化训练时，我们可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，**单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差**。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。\n",
    "\n",
    "即每一个mini-batch进入，都会更新一次该BN层的moving_mean和moving_std。当所有mini-batch都进入之后，全部完成更新后，就有了预测模式里，该BN层的均值和方差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从零实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:35:50.078483Z",
     "start_time": "2020-04-09T06:35:47.908328Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:35:50.152900Z",
     "start_time": "2020-04-09T06:35:50.146651Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    if not is_training:\n",
    "        X_hat = (X - moving_mean) / np.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用FC层\n",
    "            mean = X.mean(axis=0)\n",
    "            var = ((X - mean)**2).mean(axis=0)\n",
    "            \n",
    "        else:\n",
    "            # 使用Conv2D\n",
    "            mean = X.mean(axos=(0, 2, 3), keepdims=True)\n",
    "            var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)\n",
    "            \n",
    "        # 标准化\n",
    "        X_hat = (X - mean) / np.sqrt(var + eps)\n",
    "        \n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "        \n",
    "    # 拉伸和平移\n",
    "    Y = gamma * X_hat + beta\n",
    "    return Y, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们自定义一个BatchNorm层。它保存参与求梯度和迭代的拉伸参数gamma和偏移参数beta，同时也维护移动平均得到的均值和方差，以便能够在模型预测时被使用。BatchNorm实例所需指定的num_features参数对于全连接层来说应为输出个数，对于卷积层来说则为输出通道数。该实例所需指定的num_dims参数对于全连接层和卷积层来说分别为2和4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:35:50.237735Z",
     "start_time": "2020-04-09T06:35:50.227568Z"
    }
   },
   "outputs": [],
   "source": [
    "class BatchNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, decay=0.9, epsilon=1e-5, **kwargs):\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        super(BatchNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma',\n",
    "                                     shape=[input_shape[-1], ],\n",
    "                                     initializer=tf.initializers.ones,\n",
    "                                     trainable=True)\n",
    "        self.beta = self.add_weight(name='beta',\n",
    "                                    shape=[input_shape[-1], ],\n",
    "                                    initializer=tf.initializers.zeros,\n",
    "                                    trainable=True)\n",
    "        self.moving_mean = self.add_weight(name='moving_mean',\n",
    "                                           shape=[input_shape[-1], ],\n",
    "                                           initializer=tf.initializers.zeros,\n",
    "                                           trainable=False)\n",
    "        self.moving_variance = self.add_weight(name='moving_variance',\n",
    "                                               shape=[input_shape[-1], ],\n",
    "                                               initializer=tf.initializers.ones,\n",
    "                                               trainable=False)\n",
    "        # super(BatchNormalization, self).build(input_shape)\n",
    "\n",
    "    def assign_moving_average(self, variable, value):\n",
    "        \"\"\"\n",
    "        variable = variable * decay + value * (1 - decay)\n",
    "        \"\"\"\n",
    "        delta = variable * self.decay + value * (1 - self.decay)\n",
    "        return variable.assign(delta)\n",
    "\n",
    "    # @tf.function\n",
    "    def call(self, inputs, training):\n",
    "        if training:\n",
    "            # tf.nn.moments -> 求出inputs延axes位置的均值和方差\n",
    "            batch_mean, batch_variance = tf.nn.moments(x=inputs, axes=list(range(len(inputs.shape) - 1)))\n",
    "            \n",
    "            # 求出该轮batch对整体mean和variance的影响\n",
    "            mean_update = self.assign_moving_average(self.moving_mean, batch_mean)\n",
    "            variance_update = self.assign_moving_average(self.moving_variance, batch_variance)\n",
    "            self.add_update(mean_update)\n",
    "            self.add_update(variance_update)\n",
    "            mean, variance = batch_mean, batch_variance\n",
    "        else:\n",
    "            mean, variance = self.moving_mean, self.moving_variance\n",
    "        output = tf.nn.batch_normalization(inputs,\n",
    "                                           mean=mean,\n",
    "                                           variance=variance,\n",
    "                                           offset=self.beta,\n",
    "                                           scale=self.gamma,\n",
    "                                           variance_epsilon=self.epsilon)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在LeNet里使用BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:35:50.323062Z",
     "start_time": "2020-04-09T06:35:50.302996Z"
    }
   },
   "outputs": [],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=6,kernel_size=5),\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "    tf.keras.layers.Conv2D(filters=16,kernel_size=5),\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120),\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(84),\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(10,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:36:42.270066Z",
     "start_time": "2020-04-09T06:35:50.391896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 10s 217us/sample - loss: 0.4140 - accuracy: 0.9358 - val_loss: 0.1827 - val_accuracy: 0.9516\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 10s 209us/sample - loss: 0.0886 - accuracy: 0.9746 - val_loss: 0.0943 - val_accuracy: 0.9735\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 10s 210us/sample - loss: 0.0667 - accuracy: 0.9801 - val_loss: 0.0780 - val_accuracy: 0.9773\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 10s 205us/sample - loss: 0.0555 - accuracy: 0.9830 - val_loss: 0.0791 - val_accuracy: 0.9766\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 10s 204us/sample - loss: 0.0488 - accuracy: 0.9850 - val_loss: 0.1249 - val_accuracy: 0.9636\n",
      "10000/10000 - 1s - loss: 0.1107 - accuracy: 0.9640\n",
      "Test loss: 0.1107352244278416\n",
      "Test accuracy: 0.964\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "net.compile(loss='sparse_categorical_crossentropy',\n",
    "            optimizer=tf.keras.optimizers.RMSprop(),\n",
    "            metrics=['accuracy'])\n",
    "history = net.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=5,\n",
    "                    validation_split=0.2)\n",
    "test_scores = net.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:36:42.430736Z",
     "start_time": "2020-04-09T06:36:42.426338Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:36:42.621616Z",
     "start_time": "2020-04-09T06:36:42.603201Z"
    }
   },
   "outputs": [],
   "source": [
    "# 构建网络\n",
    "# 因为BN应用在层与激活函数之间，所以在定义Conv2D时不指定activation，在应用了BN之后再添加Activation\n",
    "\n",
    "net = tf.keras.Sequential([\n",
    "    Conv2D(filters=6, kernel_size=5),\n",
    "    BatchNormalization(),\n",
    "    Activation('sigmoid'),\n",
    "    MaxPool2D(pool_size=2, strides=2),\n",
    "    Conv2D(filters=16, kernel_size=5),\n",
    "    BatchNormalization(),\n",
    "    Activation('sigmoid'),\n",
    "    MaxPool2D(pool_size=2, strides=2),\n",
    "    Flatten(),\n",
    "    Dense(120),\n",
    "    BatchNormalization(),\n",
    "    Activation('sigmoid'),\n",
    "    Dense(84),\n",
    "    BatchNormalization(),\n",
    "    Activation('sigmoid'),\n",
    "    Dense(10, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:37:44.809437Z",
     "start_time": "2020-04-09T06:36:42.768179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 13s 275us/sample - loss: 0.4268 - accuracy: 0.9296 - val_loss: 0.4279 - val_accuracy: 0.8472\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 12s 245us/sample - loss: 0.0847 - accuracy: 0.9762 - val_loss: 0.1901 - val_accuracy: 0.9438\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 11s 238us/sample - loss: 0.0625 - accuracy: 0.9812 - val_loss: 0.6341 - val_accuracy: 0.7740\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 12s 250us/sample - loss: 0.0501 - accuracy: 0.9842 - val_loss: 0.1043 - val_accuracy: 0.9674\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 12s 254us/sample - loss: 0.0441 - accuracy: 0.9863 - val_loss: 0.0690 - val_accuracy: 0.9796\n",
      "10000/10000 - 1s - loss: 0.0623 - accuracy: 0.9809\n",
      "Test loss: 0.062268997953645884\n",
      "Test accuracy: 0.9809\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "net.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "history = net.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=5,\n",
    "                    validation_split=0.2)\n",
    "test_scores = net.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet - 残差网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理论上来说，NN越深说明可供调整的参数越多，从而网络对特征的抽象程度要更高，但是实际上不是这样的。梯度消失/爆炸、过拟合、退化问题(如CNN每一层Conv都会对信息进行有损压缩，多次卷积会导致信息丢失，从而使得训练误差变大导致误差问题)。所以在实践中添加过多层之后训练误差往往不升反降。而残差网络就是用于解决这个问题的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "左图中为一各普通的线性网络结构。右图中为残差块，即增加了跨层的数据通道，使得输入可以通过跨层的数据线路更快的向前传播。\n",
    "\n",
    "注: 在ResNet的后续版本中，残差块的 ConV+BN+ReLU 的结构被改成了 BN+ReLU+ConV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/class8_2.1.svg\" style=\"zoom:100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:37:44.948336Z",
     "start_time": "2020-04-09T06:37:44.946057Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 残差块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下述残差块为Y = X -> Conv -> BN -> ReLU -> Conv -> BN; output = Y + X\n",
    "\n",
    "然后对于output中的X可以视情况选择对其做1x1的Conv，也可以不做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:47:05.059535Z",
     "start_time": "2020-04-09T06:47:05.026599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 6, 6, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Residual(tf.keras.Model):\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.conv1 = layers.Conv2D(num_channels,\n",
    "                                   padding='same',\n",
    "                                   kernel_size=3,\n",
    "                                   strides=strides)\n",
    "        self.conv2 = layers.Conv2D(num_channels, \n",
    "                                   kernel_size=3,\n",
    "                                   padding='same')\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = layers.Conv2D(num_channels,\n",
    "                                       kernel_size=1,\n",
    "                                       strides=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "    def call(self, X):\n",
    "        Y = activations.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        return activations.relu(Y + X)\n",
    "\n",
    "# 残差块output的形状\n",
    "# 因为残差块内Conv为same, 所以输出的形状不变。\n",
    "blk = Residual(num_channels=3, use_1x1conv=True)\n",
    "X = tf.random.uniform((4, 6, 6, 3))\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 残差网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ResNetBlock`可以通过`num_residuals`来决定几个残差块相连。且第一个残差块中跨层连接为1x1Conv且strides=2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:47:18.424674Z",
     "start_time": "2020-04-09T06:47:18.418674Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResnetBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,num_channels, num_residuals, first_block=False,**kwargs):\n",
    "        super(ResnetBlock, self).__init__(**kwargs)\n",
    "        self.listLayers=[]\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                self.listLayers.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                self.listLayers.append(Residual(num_channels))      \n",
    "\n",
    "    def call(self, X):\n",
    "        for layer in self.listLayers.layers:\n",
    "            X = layer(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:47:35.756885Z",
     "start_time": "2020-04-09T06:47:35.704607Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self,num_blocks,**kwargs):\n",
    "        super(ResNet, self).__init__(**kwargs)\n",
    "        self.conv=layers.Conv2D(64, kernel_size=7, strides=2, padding='same')\n",
    "        self.bn=layers.BatchNormalization()\n",
    "        self.relu=layers.Activation('relu')\n",
    "        self.mp=layers.MaxPool2D(pool_size=3, strides=2, padding='same')\n",
    "        self.resnet_block1=ResnetBlock(64,num_blocks[0], first_block=True)\n",
    "        self.resnet_block2=ResnetBlock(128,num_blocks[1])\n",
    "        self.resnet_block3=ResnetBlock(256,num_blocks[2])\n",
    "        self.resnet_block4=ResnetBlock(512,num_blocks[3])\n",
    "        self.gap=layers.GlobalAvgPool2D()\n",
    "        self.fc=layers.Dense(units=10,activation=tf.keras.activations.softmax)\n",
    "\n",
    "    def call(self, x):\n",
    "        x=self.conv(x)\n",
    "        x=self.bn(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.mp(x)\n",
    "        x=self.resnet_block1(x)\n",
    "        x=self.resnet_block2(x)\n",
    "        x=self.resnet_block3(x)\n",
    "        x=self.resnet_block4(x)\n",
    "        x=self.gap(x)\n",
    "        x=self.fc(x)\n",
    "        return x\n",
    "\n",
    "res_net=ResNet([2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:47:37.579808Z",
     "start_time": "2020-04-09T06:47:37.350685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:\t (1, 224, 224, 1)\n",
      "conv2d_70 output shape:\t (1, 112, 112, 64)\n",
      "batch_normalization_63 output shape:\t (1, 112, 112, 64)\n",
      "activation_11 output shape:\t (1, 112, 112, 64)\n",
      "max_pooling2d_7 output shape:\t (1, 56, 56, 64)\n",
      "resnet_block_12 output shape:\t (1, 56, 56, 64)\n",
      "resnet_block_13 output shape:\t (1, 28, 28, 128)\n",
      "resnet_block_14 output shape:\t (1, 14, 14, 256)\n",
      "resnet_block_15 output shape:\t (1, 7, 7, 512)\n",
      "global_average_pooling2d_3 output shape:\t (1, 512)\n",
      "dense_9 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# 查看网络各层的shape\n",
    "res_net = ResNet([2,2,2,2])\n",
    "X = tf.random.uniform(shape=(1, 224, 224, 1))\n",
    "print('input shape:\\t', X.shape)\n",
    "for layer in res_net.layers:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T09:03:11.387255Z",
     "start_time": "2020-04-09T09:03:04.729106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 6s - loss: 0.3930 - accuracy: 0.8598\n"
     ]
    }
   ],
   "source": [
    "# 获取数据+训练模型+验证模型\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "res_net.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# res_net.fit(x_train, y_train,\n",
    "#             batch_size=64,\n",
    "#             epochs=5,\n",
    "#             validation_split=0.2)\n",
    "\n",
    "# 因为CPU训练要很久，所以用GPU训练完了把weights拿过来\n",
    "res_net.load_weights('files/class8_2_weights.h5')\n",
    "\n",
    "# 评估模型\n",
    "test_scores = res_net.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet - 稠密网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet是ResNet的一种变体，其主要区别如下图。图左为ResNet, 图右为DenseNet。ResNet的跨层连接使用相加，而DenseNet的跨层连接使用从cat。\n",
    "\n",
    "DenseNet的主要构成是稠密块(dense block)和过渡层(transition layer)。前者定义了输入和输出是如何连接的，后者用来控制通道数，使之不过大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/class8_3.1.svg\" style=\"zoom:100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T03:15:26.516407Z",
     "start_time": "2020-04-30T03:15:22.529630Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Dropout, Activation, concatenate, AveragePooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BottleNeck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T03:15:26.536999Z",
     "start_time": "2020-04-30T03:15:26.529681Z"
    }
   },
   "outputs": [],
   "source": [
    "# 在`BottleNeck`中实现 “BN+Activation+Conv”结构。(即2.1中提到的ResNet改良版的结构)\n",
    "\n",
    "class BottleNeck(tf.keras.layers.Layer):\n",
    "    def __init__(self, growth_rate, drop_rate):\n",
    "        super(BottleNeck, self).__init__()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=4 * growth_rate,\n",
    "                                            kernel_size=(1, 1),\n",
    "                                            strides=1,\n",
    "                                            padding=\"same\")\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=growth_rate,\n",
    "                                            kernel_size=(3, 3),\n",
    "                                            strides=1,\n",
    "                                            padding=\"same\")\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=drop_rate)\n",
    "\n",
    "        self.listLayers = [self.bn1,\n",
    "                           tf.keras.layers.Activation(\"relu\"),\n",
    "                           self.conv1,\n",
    "                           self.bn2,\n",
    "                           tf.keras.layers.Activation(\"relu\"),\n",
    "                           self.conv2,\n",
    "                           self.dropout]\n",
    "\n",
    "    def call(self, x):\n",
    "        y = x\n",
    "        for layer in self.listLayers.layers:\n",
    "            y = layer(y)\n",
    "        y = tf.keras.layers.concatenate([x,y], axis=-1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T03:15:26.554603Z",
     "start_time": "2020-04-30T03:15:26.549705Z"
    }
   },
   "outputs": [],
   "source": [
    "# DenseNet由Bottleneck组成，每块使用相同的输出通道数\n",
    "\n",
    "class DenseBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, growth_rate, drop_rate=0.5):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.growth_rate = growth_rate\n",
    "        self.drop_rate = drop_rate\n",
    "        self.listLayers = []\n",
    "        for _ in range(num_layers):\n",
    "            self.listLayers.append(BottleNeck(growth_rate=self.growth_rate, drop_rate=self.drop_rate))\n",
    "\n",
    "    def call(self, x):\n",
    "        for layer in self.listLayers.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T03:15:57.000615Z",
     "start_time": "2020-04-30T03:15:56.954914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 8, 23)\n"
     ]
    }
   ],
   "source": [
    "# 下例中，定义一个有2个BottleNeck相连，且每个BottleNeck的growth_rate为10的DenseBlock\n",
    "# 使用通道数为3的输入的时候，会得到通道数为 3 + 2 * 10 = 23 的输出。卷积块的通道数控住了输出通道数相当于输入通道数的增长\n",
    "# 因此也被称为增长率(growth rate)\n",
    "# 可以理解为每个BottleNeck的输出通道数为其 (输入的通道数+growth_rate)\n",
    "\n",
    "blk = DenseBlock(2, 10)\n",
    "X = tf.random.uniform((4, 8, 8, 3))\n",
    "Y = blk(X)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T09:40:03.287756Z",
     "start_time": "2020-04-09T09:40:03.281425Z"
    }
   },
   "outputs": [],
   "source": [
    "# 由于每个DenseBlock都会带来通道数的增加，使用过多会带来过于复杂的模型。所以使用过渡层来控制模型复杂度。\n",
    "# 过渡层使用1x1卷积层来减少通道数，并使用strides=2的平均池化层减半高和宽，从而进一步降低模型复杂度\n",
    "\n",
    "class TransitionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_channels):\n",
    "        super(TransitionLayer, self).__init__()\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters=out_channels,\n",
    "                                           kernel_size=(1, 1),\n",
    "                                           strides=1,\n",
    "                                           padding=\"same\")\n",
    "        self.pool = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),\n",
    "                                                     strides=2,\n",
    "                                                     padding=\"same\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.bn(inputs)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T09:40:03.466516Z",
     "start_time": "2020-04-09T09:40:03.451419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 4, 4, 10])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将3.2.1中的Y输入进过渡层，查看通道数变化\n",
    "# shape变为原来的一半(因为AvgPool中strides为2); channels变为10, 因为out_channels被设定为10\n",
    "\n",
    "blk = TransitionLayer(10)\n",
    "blk(Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T09:40:04.608118Z",
     "start_time": "2020-04-09T09:40:04.517337Z"
    }
   },
   "outputs": [],
   "source": [
    "# num_init_features是网络里第一个Conv的filters的数量\n",
    "# growth_rate是dense_block内所有BottleNeck的growth_rate(即每经过一个BottleNeck增加的通道数)\n",
    "# compression_rate是TransitionLayer内对DenseBlock通道数的压缩比例\n",
    "# # 例如compression_rate=0.5的时候，若DenseBlock的输出通道数为a, 则在经过TransitionLayer之后, 其通道数为0.5a\n",
    "# block_layers里面的元素, 是每一个DenseBlock内BottleNeck的数量\n",
    "# drop_rate是所有DenseBlock内所有DropOut层的drop概率\n",
    "\n",
    "class DenseNet(tf.keras.Model):\n",
    "    def __init__(self, num_init_features, growth_rate, block_layers, compression_rate, drop_rate):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters=num_init_features,\n",
    "                                           kernel_size=(7, 7),\n",
    "                                           strides=2,\n",
    "                                           padding=\"same\")\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        self.pool = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\n",
    "                                              strides=2,\n",
    "                                              padding=\"same\")\n",
    "        self.num_channels = num_init_features\n",
    "        self.dense_block_1 = DenseBlock(num_layers=block_layers[0], growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "        self.num_channels += growth_rate * block_layers[0]\n",
    "        self.num_channels = compression_rate * self.num_channels\n",
    "        self.transition_1 = TransitionLayer(out_channels=int(self.num_channels))\n",
    "        self.dense_block_2 = DenseBlock(num_layers=block_layers[1], growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "        self.num_channels += growth_rate * block_layers[1]\n",
    "        self.num_channels = compression_rate * self.num_channels\n",
    "        self.transition_2 = TransitionLayer(out_channels=int(self.num_channels))\n",
    "        self.dense_block_3 = DenseBlock(num_layers=block_layers[2], growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "        self.num_channels += growth_rate * block_layers[2]\n",
    "        self.num_channels = compression_rate * self.num_channels\n",
    "        self.transition_3 = TransitionLayer(out_channels=int(self.num_channels))\n",
    "        self.dense_block_4 = DenseBlock(num_layers=block_layers[3], growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "\n",
    "        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.fc = tf.keras.layers.Dense(units=10,\n",
    "                                        activation=tf.keras.activations.softmax)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.dense_block_1(x)\n",
    "        x = self.transition_1(x)\n",
    "        x = self.dense_block_2(x)\n",
    "        x = self.transition_2(x)\n",
    "        x = self.dense_block_3(x)\n",
    "        x = self.transition_3(x,)\n",
    "        x = self.dense_block_4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "dense_net = DenseNet(num_init_features=64, \n",
    "                     growth_rate=32, \n",
    "                     block_layers=[4,4,4,4], \n",
    "                     compression_rate=0.5, \n",
    "                     drop_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T09:40:05.515231Z",
     "start_time": "2020-04-09T09:40:05.312358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_333 output shape:\t (1, 48, 48, 64)\n",
      "batch_normalization_323 output shape:\t (1, 48, 48, 64)\n",
      "max_pooling2d_21 output shape:\t (1, 24, 24, 64)\n",
      "dense_block_28 output shape:\t (1, 24, 24, 192)\n",
      "transition_layer_20 output shape:\t (1, 12, 12, 96)\n",
      "dense_block_29 output shape:\t (1, 12, 12, 224)\n",
      "transition_layer_21 output shape:\t (1, 6, 6, 112)\n",
      "dense_block_30 output shape:\t (1, 6, 6, 240)\n",
      "transition_layer_22 output shape:\t (1, 3, 3, 120)\n",
      "dense_block_31 output shape:\t (1, 3, 3, 248)\n",
      "global_average_pooling2d_9 output shape:\t (1, 248)\n",
      "dense_15 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# 检查输出shape\n",
    "X = tf.random.uniform(shape=(1, 96, 96, 1))\n",
    "for layer in dense_net.layers:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T09:40:15.845325Z",
     "start_time": "2020-04-09T09:40:09.005599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 5s - loss: 0.5964 - accuracy: 0.8231\n"
     ]
    }
   ],
   "source": [
    "# 获取数据+训练模型+验证模型\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "dense_net.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# dense_net.fit(x_train, y_train,\n",
    "#               batch_size=64,\n",
    "#               epochs=5,\n",
    "#               validation_split=0.2)\n",
    "\n",
    "# 因为CPU训练要很久，所以用GPU训练完了把weights拿过来\n",
    "dense_net.load_weights('files/class8_3_weights.h5')\n",
    "\n",
    "# 评估模型\n",
    "test_scores = dense_net.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重点总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BN\n",
    "    - BN能使得每一层的输出更加的稳定。(深隐层的输出不加BN会剧烈变化,导致训练不出好模型)\n",
    "    - 在训练的时候\n",
    "        - 对该层的output求其mean与var\n",
    "        - Y_tmp = (output - mean) / sqrt(var + epsilon)。做标准化, 其中epsilon是一个很小的正数，防止分母是0。\n",
    "        - Y = gamma * Y_tmp + beta。其中gamma与beta是两个超参数, 分别表示拉伸和平移。是训练出来的。\n",
    "        - 然后对Y应用激活函数，将其 传输给下一层, 本次epoch本batch训练的BN层作用完成\n",
    "    - 在预测的时候\n",
    "        - 仍然要在BN层时对该层的输出做标准化，只不过用的不是该层output的mean与var，而是之前训练时留下的moving_mean和moving_var\n",
    "            - 在训练开始初始化模型的时候，会将该BN层的moving_mean和moving_var初始化为0。之后每一个batch的数据进入该层的时候\n",
    "            - moving_mean = momentum * moving_mean + (1 - momentum) * 本次训练output的mean\n",
    "            - moving_var = momentum * moving_var + (1 - momentum) * 本次训练output的var\n",
    "            - 其中momentum为本次移动窗口的权重，即该值越大，本次batch的mean和var对移动平均和移动方差的影响越小。在案例中使用的momentum是0.9\n",
    "        - Y_tmp = (output - moving_mean) / sqrt(moving_var + epsilon)。做标准化, 其中epsilon是一个很小的正数，防止分母是0。\n",
    "        - Y = gamma * Y_tmp + beta。其中gamma与beta是之前在训练的时候训练好的参数\n",
    "        - 然后对Y应用激活函数，将其传输给下一层\n",
    "    - 结论\n",
    "        - 在训练的使用了BN，那么在预测的时候也要按之前训练时的batch_size将sample输入网络，而不是一次性全部输入网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ResNet\n",
    "    - 残差网络用于解决深层NN中的退化问题(即将很多层之前的input的信息跨通道传递到本层)\n",
    "    - ResBlock\n",
    "        - Y = X -> Conv -> BN -> ReLU -> Conv -> BN; output = Y + X\n",
    "    - ResNet\n",
    "        - Conv + BN + Activation + MaxPool + ResBlock*4 + GlobalAvgPool + Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DenseNet\n",
    "    - BottleNeck\n",
    "        - Y = X -> BN -> ReLU -> Conv(filters=4\\*growth_rate) -> BN -> ReLU -> Conv(filters=growth_rate) -> DropOut; output = [X, Y]\n",
    "    - DenseBlock\n",
    "        - n * BottleNeck\n",
    "    - TransitionLayer\n",
    "        - BN -> ReLU -> 1x1Conv(filters=input_channels*compression_rate) -> AvgPool\n",
    "    - DenseNet\n",
    "        - Conv -> BN -> ReLU -> MaxPool -> (DenseBlock + TransitionLayer) * 3 -> DenseBlock -> GlobalAvgPool -> Dense\n",
    "    - 需要注意DenseNet里的growth_rate与compression_rate\n",
    "        - 在一个DenseBlock内有多个BottleNeck, 每个BottleNeck都有相同的growth_rate, 即经过每层BottleNeck, 输出的通道数增加的数量。例如输入的通道数为a, growth_rate=b, 则经过一个BottleNeck其通道数为a+b\n",
    "        - 在TransitionLayer中有一个compression_rate, 表示将输入的通道数压缩的比例。例如输入的通道数为a, compression_rate=0.5, 则经过TransitionLayer输出的通道数为0.5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
