{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T09:17:29.750796Z",
     "start_time": "2020-03-19T09:17:27.395936Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二维卷积层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二维卷积计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举例说明卷积计算。如下图所示，输入为一个shape=(3,3)的数组。kernel是一个shape=(2,2)的数组，kernel又被称为卷积核或者过滤器(filter)。卷积核窗口(卷积窗口)的形状取决于kernel的高和宽。在二维卷积计算中，卷积窗口从输入数组的最左上方开始，按从左往右，从上往下的顺序，一次在输入数组中滑动，对卷积窗口中的输入子数组与kernel相乘并求和，得到输出数组中相应位置的元素。\n",
    "\n",
    "$$\n",
    "0\\times0+1\\times1+3\\times2+4\\times3=19,\\\\1\\times0+2\\times1+4\\times2+5\\times3=25,\\\\3\\times0+4\\times1+6\\times2+7\\times3=37,\\\\4\\times0+5\\times1+7\\times2+8\\times3=43.\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/class6_1.1.svg\" style=\"zoom:100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T10:07:42.851338Z",
     "start_time": "2020-03-19T10:07:42.845915Z"
    }
   },
   "outputs": [],
   "source": [
    "# 二维卷积计算（cross-correlation）从零实现\n",
    "\n",
    "def corr2d(X, K):\n",
    "    # kernel shape\n",
    "    h,w = K.shape\n",
    "    \n",
    "    # 输出的shape\n",
    "    Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)))\n",
    "    \n",
    "    # 进行卷积计算\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j].assign(tf.cast(tf.reduce_sum(X[i:i+h, j:j+w] * K), dtype=tf.float32))\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T09:47:34.003949Z",
     "start_time": "2020-03-19T09:47:33.995204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
       "array([[19., 25.],\n",
       "       [37., 43.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.constant([[0,1,2], [3,4,5], [6,7,8]])\n",
    "K = tf.constant([[0,1], [2,3]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2d - 二维卷积层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二维卷积层将输入和卷积层做cross-correlation运算，然后加上一个标量bias得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。\n",
    "\n",
    "下面基于`corr2d`函数，实现一个自定义的二维卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T10:12:36.993998Z",
     "start_time": "2020-03-19T10:12:36.988951Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, kernel_size):\n",
    "        self.w = self.add_weight(name='w',\n",
    "                                 shape=kernel_size,\n",
    "                                 initializer=tf.random_normal_initializer())\n",
    "        self.b = self.add_weight(name='b',\n",
    "                                 shape=(1,),\n",
    "                                 initializer=tf.random_normal_initializer())\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return corr2d(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Detectation - 边缘检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积层的简单应用: 检测图像中物体的边缘(像素变化的位置)。首先构造一张6x8的图像，它中间4列为黑(0), 其余为白(1)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T07:28:38.702111Z",
     "start_time": "2020-03-23T07:28:38.694736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(6, 8) dtype=float32, numpy=\n",
       "array([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.Variable(tf.ones((6,8)))\n",
    "X[:, 2:6].assign(tf.zeros(X[:, 2:6].shape))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造高和宽分别为1和2的卷积核, 并对其做卷积计算。发现白和黑的边缘分别被检测成了1和-1，其余部分的输出全是0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T07:28:39.121144Z",
     "start_time": "2020-03-23T07:28:39.097940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(6, 7) dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = tf.constant([[1,-1]], dtype=tf.float32)\n",
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## leran kernel by data - 迭代卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T07:40:05.091259Z",
     "start_time": "2020-03-23T07:40:05.063370Z"
    }
   },
   "outputs": [],
   "source": [
    "# 用1.3的例子生成X和Y\n",
    "X = tf.Variable(tf.ones((6,8)))\n",
    "X[:, 2:6].assign(tf.zeros(X[:, 2:6].shape))\n",
    "\n",
    "K = tf.constant([[1,-1]], dtype=tf.float32)\n",
    "Y = corr2d(X, K)\n",
    "\n",
    "X = tf.reshape(X, (1,6,8,1)) # 相当于Flatten()\n",
    "Y = tf.reshape(Y, (1,6,7,1)) # 相当于Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T07:48:01.427588Z",
     "start_time": "2020-03-23T07:48:01.375309Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2, loss 19.553\n",
      "batch 4, loss 7.277\n",
      "batch 6, loss 2.858\n",
      "batch 8, loss 1.150\n",
      "batch 10, loss 0.468\n",
      "batch 12, loss 0.191\n",
      "batch 14, loss 0.078\n",
      "batch 16, loss 0.032\n",
      "batch 18, loss 0.013\n",
      "batch 20, loss 0.005\n"
     ]
    }
   ],
   "source": [
    "# 对conv2d进行迭代\n",
    "# 只有weights无bias\n",
    "\n",
    "conv2d = tf.keras.layers.Conv2D(1, (1,2))\n",
    "Y_hat = conv2d(X)\n",
    "\n",
    "for i in range(20):\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as g:\n",
    "        g.watch(conv2d.weights[0])\n",
    "        Y_hat = conv2d(X)\n",
    "        l = (abs(Y_hat - Y)) ** 2\n",
    "        dl = g.gradient(target=l, source=conv2d.weights[0])\n",
    "        lr = 3e-2\n",
    "        update = tf.multiply(lr, dl)\n",
    "        updated_weights = conv2d.get_weights()\n",
    "        updated_weights[0] = conv2d.weights[0] - update\n",
    "        conv2d.set_weights(updated_weights)  \n",
    "\n",
    "        if (i + 1)% 2 == 0:\n",
    "            print('batch %d, loss %.3f' % (i + 1, tf.reduce_sum(l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Map and Receptive Field - 特征图和感受野"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二维卷积层输出的二维数组叫做特征图，因为可以将它看做是输入在空间维度上某一级的表征。\n",
    "\n",
    "影响元素x的正向传播的所有可能输入区域(可能大于输入的实际尺寸)叫做x的感受野。以1.1中图为例，输入中阴影部分的四个元素为输出中阴影部分元素的感受野。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and Stride - 填充和步幅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在1.1的例子中，input_shape = (3,3), kernel_shape = (2,2), 计算得到了output_shape = (2,2)。一般来说，假设input_shape =$(n_h, n_w)$, kernel_shape = $(k_h, k_w)$, 那么output_shape = $(n_h - k_h + 1, n_w - k_w + 1)$\n",
    "\n",
    "所以，卷积层的输出形状由输入形状和卷积核窗口形状决定的。在2.中介绍卷积层的两个超参数，填充和步幅。这两个超参数可以在给定输入和卷积核的情况下改变输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding - 填充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "填充是指在输入的高和宽的两侧填充元素(通常填充0)。下图中我们在1.1例子的图的两侧分别添加了值为0的元素，从而使得输入的高和宽从3变成5，并导致输出的高和宽由2增加到了4。该图片相当于padding_shape=(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T08:26:16.821260Z",
     "start_time": "2020-03-23T08:26:16.816963Z"
    }
   },
   "source": [
    "<img src=\"img/class6_2.1.svg\" style=\"zoom:100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以当padding_shape = $(p_h, p_w)$时，则output_shape = $(n_h + p_h - k_h + 1, n_w + p_w - k_w + 1)$。 也就是说，输出的高和宽会分别增加 $p_h$ 和 $p_w$\n",
    "\n",
    "很多情况下，我们会设置 $p_h = k_h - 1$ 和 $p_w = k_w - 1$ 来使得输入和输出具有相同的高和宽。这样会方便在构造网络时推测每个层的输出形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "备注: 这里的padding_shape和外面大家的说法不太一样。一般而言如果padding='same'; 在我们这的定义是pad = kernel - 1; 在外面的定义为pad = (kernel - 1) / 2;\n",
    "\n",
    "即外面的padding讲的是在外面围几圈，而我们这的padding讲的是加几行/几列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride - 步幅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T08:44:34.191973Z",
     "start_time": "2020-03-23T08:44:34.187557Z"
    }
   },
   "source": [
    "默认情况下，stride=(1,1)， 即每次移动kernel在高上移动一格，在宽上移动一格。下图中展示了，stride=(3,2)的步幅移动形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/class6_2.2.svg\" style=\"zoom:100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以当stride = $(s_h, s_w)$ 时。则output_shape = $((n_h - k_h + p_h + s_h)/s_h, (n_w - k_w + p_h + s_w)/s_w)$ (若上述式子中计算出来为小数，则对该数进行int()，即忽略小数位，只取整数位)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T09:35:56.385708Z",
     "start_time": "2020-03-23T09:35:56.380460Z"
    }
   },
   "outputs": [],
   "source": [
    "# 生成X\n",
    "\n",
    "X = tf.Variable(tf.ones((6,8)))\n",
    "X[:, 2:6].assign(tf.zeros(X[:, 2:6].shape))\n",
    "X = tf.reshape(X, (1,6,8,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T09:35:44.074905Z",
     "start_time": "2020-03-23T09:35:44.065321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 1, 1])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# h = int((6 - 3 + 0 + 3) / 3) = 2\n",
    "# w = int((5 - 4 + 0 + 4) / 4) = 1\n",
    "# 当使用padding='valid'的时候，默认p_h = 0 , p_w = 0\n",
    "\n",
    "conv2d = tf.keras.layers.Conv2D(filters=1,         # filters的数量\n",
    "                                kernel_size=(3,5), # kernel的shape\n",
    "                                padding='valid',   # padding的方式\n",
    "                                strides=(3,4))     # stride的shape\n",
    "conv2d(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T09:37:22.090232Z",
     "start_time": "2020-03-23T09:37:22.081397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 2, 1])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# h = int((6 - 3 + 3 - 1 + 3) / 3) = 2\n",
    "# w = int((5 - 4 + 5 - 1 + 4) / 4) = 2\n",
    "# 当使用padding='same'的时候，默认p_h = k_h -1 , p_w = k_w - 1\n",
    "\n",
    "conv2d = tf.keras.layers.Conv2D(filters=1,         # filters的数量\n",
    "                                kernel_size=(3,5), # kernel的shape\n",
    "                                padding='same',    # padding的方式\n",
    "                                strides=(3,4))     # stride的shape\n",
    "conv2d(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T09:40:05.339186Z",
     "start_time": "2020-03-23T09:40:05.330741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1, 1, 1])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果strides过大，就只会对第一个感受野做卷积计算。\n",
    "\n",
    "conv2d = tf.keras.layers.Conv2D(filters=1,         # filters的数量\n",
    "                                kernel_size=(3,5), # kernel的shape\n",
    "                                padding='valid',    # padding的方式\n",
    "                                strides=(10,10))     # stride的shape\n",
    "conv2d(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多输入通道和多输出通道"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面两节里我们用到的输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3\\times h\\times w$的多维数组。我们将大小为3的这一维称为通道（channel）维。本节我们将介绍含多个输入通道或多个输出通道的卷积核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    if len(X.shape) <= 1:\n",
    "        X = tf.reshape(X, (X.shape[0],1))\n",
    "    Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w +1)))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j].assign(tf.cast(tf.reduce_sum(X[i:i+h, j:j+w] * K), dtype=tf.float32))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多通道输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T10:09:43.135868Z",
     "start_time": "2020-03-23T10:09:43.131900Z"
    }
   },
   "source": [
    "当输入数据含多个通道, 我们需要构造一个输入通道数与输入数据的通道数相同的卷积核, 这样才能做合适的计算.\n",
    "\n",
    "假设输入数据的通道数为$c_i$，那么卷积核的输入通道数同样为$c_i$。那么第i个通道的输入数据会与第i个通道的卷积核进行卷积计算，然后我们会获得i个shape相同的输出，将这i个输出做元素上的加法,获得最终的输出。具体入下图案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/class6_3.1.svg\" style=\"zoom:100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T07:56:53.861251Z",
     "start_time": "2020-03-24T07:56:53.847303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 56.,  72.],\n",
       "       [104., 120.]], dtype=float32)>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对每个通道进行互相关运算，然后进行累加\n",
    "\n",
    "def corr2d_multi_in(X, K):\n",
    "    return tf.reduce_sum([corr2d(X[i], K[i]) for i in range(X.shape[0])],axis=0)\n",
    "\n",
    "X = tf.constant([[[0,1,2],[3,4,5],[6,7,8]],\n",
    "                 [[1,2,3],[4,5,6],[7,8,9]]])\n",
    "K = tf.constant([[[0,1],[2,3]],\n",
    "                 [[1,2],[3,4]]])\n",
    "\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多通道输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为1。设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i\\times k_h\\times k_w$的核数组。将它们在输出通道维上连结，卷积核的形状即$c_o\\times c_i\\times k_h\\times k_w$。在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输入数组计算而来。\n",
    "\n",
    "即一个filters只能产生一个output，那么使用多个filters之后，再对其结果进行堆叠，则输出有多个channel。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/class6_3.2.PNG\" style=\"zoom:60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T07:57:33.682038Z",
     "start_time": "2020-03-24T07:57:33.654981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=\n",
       "array([[[ 56.,  72.],\n",
       "        [104., 120.]],\n",
       "\n",
       "       [[ 76., 100.],\n",
       "        [148., 172.]],\n",
       "\n",
       "       [[ 96., 128.],\n",
       "        [192., 224.]]], dtype=float32)>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    return tf.stack([corr2d_multi_in(X, k) for k in K],axis=0)\n",
    "\n",
    "X = tf.constant([[[0,1,2],[3,4,5],[6,7,8]],\n",
    "                 [[1,2,3],[4,5,6],[7,8,9]]])\n",
    "K = tf.constant([[[0,1],[2,3]],\n",
    "                 [[1,2],[3,4]]])\n",
    "K = tf.stack([K, K+1, K+2],axis=0)\n",
    "\n",
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x1 convlution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我们讨论卷积窗口形状为$1\\times 1$（$k_h=k_w=1$）的多通道卷积层。我们通常称之为$1\\times 1$卷积层，并将其中的卷积运算称为$1\\times 1$卷积。因为使用了最小窗口，$1\\times 1$卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，$1\\times 1$卷积的主要计算发生在通道维上。下图展示了使用输入通道数为3、输出通道数为2的$1\\times 1$卷积核的互相关计算。值得注意的是，**输入和输出具有相同的高和宽**。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么$1\\times 1$**卷积层的作用与全连接层等价**。\n",
    "\n",
    "**所以$1\\times 1$卷积层通常用来调整网络层之间的通道数，并控制模型复杂度**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/class6_3.3.svg\" style=\"zoom:100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T08:17:25.120830Z",
     "start_time": "2020-03-24T08:17:25.116651Z"
    }
   },
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = tf.reshape(X,(c_i, h * w))\n",
    "    K = tf.reshape(K,(c_o, c_i))\n",
    "    Y = tf.matmul(K, X)\n",
    "    return tf.reshape(Y, (c_o, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T08:17:30.316729Z",
     "start_time": "2020-03-24T08:17:30.283885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.random.uniform((3,3,3))\n",
    "K = tf.random.uniform((2,3,1,1))\n",
    "\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "\n",
    "tf.norm(Y1-Y2) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 池化层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在1.3的边缘检测中，我们通过构造卷积核从而精确的找到了像素变化的位置。设任意二维数组`X`的`i`行`j`列的元素为`X[i, j]`。如果我们构造的卷积核输出`Y[i, j]=1`，那么说明输入中`X[i, j]`和`X[i, j+1]`数值不一样。这可能意味着物体边缘通过这两个元素之间。但实际图像里，我们感兴趣的物体不会总出现在固定位置：即使我们连续拍摄同一个物体也极有可能出现像素位置上的偏移。这会导致同一个边缘对应的输出可能出现在卷积输出`Y`中的不同位置，进而对后面的模式识别造成不便。\n",
    "\n",
    "在本节中我们介绍池化（pooling）层，它的提出是为了缓解卷积层对位置的过度敏感性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D MaxPooling and AvgPooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出。不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。在二维最大池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当池化窗口滑动到某一位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。\n",
    "\n",
    "下图中展示了最大池化层的运作原理，即取出阴影范围内的最大值。平均池化层的原理类似，只不过是求阴影部分的平均值，不再赘述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/class6_4.1.svg\" style=\"zoom:100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T08:39:33.140707Z",
     "start_time": "2020-03-24T08:39:33.128919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
       "array([[4., 5.],\n",
       "       [7., 8.]], dtype=float32)>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造池化层\n",
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = tf.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w +1))\n",
    "    Y = tf.Variable(Y)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i,j].assign(tf.reduce_max(X[i:i+p_h, j:j+p_w]))\n",
    "            elif mode =='avg':\n",
    "                Y[i,j].assign(tf.reduce_mean(X[i:i+p_h, j:j+p_w]))\n",
    "    return Y\n",
    "\n",
    "# 尝试池化层\n",
    "X = tf.constant([[0,1,2],[3,4,5],[6,7,8]],dtype=tf.float32)\n",
    "pool2d(X, (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同卷积层一样，池化层可以在输入的高和宽的两侧填充并调整移动步幅来改变输出形状。其工作原理与卷积层的padding还有stride一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T07:22:57.084104Z",
     "start_time": "2020-03-25T07:22:57.078946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 4, 1), dtype=int32, numpy=\n",
       "array([[[[ 0],\n",
       "         [ 1],\n",
       "         [ 2],\n",
       "         [ 3]],\n",
       "\n",
       "        [[ 4],\n",
       "         [ 5],\n",
       "         [ 6],\n",
       "         [ 7]],\n",
       "\n",
       "        [[ 8],\n",
       "         [ 9],\n",
       "         [10],\n",
       "         [11]],\n",
       "\n",
       "        [[12],\n",
       "         [13],\n",
       "         [14],\n",
       "         [15]]]], dtype=int32)>"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 因为tf中，默认数据类型为'channels_last', 所以最后一位数为channels的数量\n",
    "# 即 (batch_size, rows, cols, channels)\n",
    "\n",
    "X = tf.reshape(tf.constant(range(16)), (1,4,4,1))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T08:50:33.206362Z",
     "start_time": "2020-03-24T08:50:33.200541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 1, 1), dtype=int32, numpy=array([[[[10]]]], dtype=int32)>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 默认情况下，`MaxPool2D`的stride与pool_size相同，所以下例中，获得了1x1的输出\n",
    "\n",
    "pool2d = tf.keras.layers.MaxPool2D(pool_size=[3,3])\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T07:23:47.019307Z",
     "start_time": "2020-03-25T07:23:47.014103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 2, 1), dtype=int32, numpy=\n",
       "array([[[[10],\n",
       "         [11]]]], dtype=int32)>"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 人工指定strides=(2,1)。即高上一次挪动两格，宽上一次移动一格\n",
    "\n",
    "pool2d = tf.keras.layers.MaxPool2D(pool_size=[3,3],\n",
    "                                   strides=[2,1])\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T06:52:07.451667Z",
     "start_time": "2020-03-25T06:52:07.445262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 2, 1), dtype=int32, numpy=\n",
       "array([[[[10],\n",
       "         [11]],\n",
       "\n",
       "        [[14],\n",
       "         [15]]]], dtype=int32)>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 人工指定padding='same', 在这个例子上即padding=3-1=2层\n",
    "# 人工指定strides=2, 即高和宽上每次都挪动2格\n",
    "# \"SAME\" tries to pad evenly left and right, but if the amount of columns to be added is odd, \n",
    "# it will add the extra column to the right, as is the case in this example \n",
    "# (the same logic applies vertically: there may be an extra row of zeros at the bottom).\n",
    "# ？？？？？？上述仍然不能解释其结果，因为其实left和right是evenly的\n",
    "\n",
    "\n",
    "pool2d = tf.keras.layers.MaxPool2D(pool_size=[3,3],\n",
    "                                   padding='same',\n",
    "                                   strides=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi-channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在处理多通道输入数据时，池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加。这意味着**池化层的输出通道数与输入通道数相等**。下面将数组X和X+1在通道维上连结来构造通道数为2的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T07:24:46.840491Z",
     "start_time": "2020-03-25T07:24:46.831539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 2, 2), dtype=int32, numpy=\n",
       "array([[[[10, 11],\n",
       "         [11, 12]],\n",
       "\n",
       "        [[14, 15],\n",
       "         [15, 16]]]], dtype=int32)>"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.reshape(tf.constant(range(16)), (1,4,4,1))\n",
    "X = tf.stack([X, X+1], axis=3)\n",
    "X = tf.reshape(X, (1,4,4,2)) # batch_size*row*col*channel\n",
    "\n",
    "# stride's shape default same as the pool_size\n",
    "pool2d = tf.keras.layers.MaxPool2D(pool_size=(3,3), \n",
    "                                   padding='valid',\n",
    "                                   strides=1,\n",
    "                                   data_format='channels_last')\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重点总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 卷积层用于提取图像的特征细节，如edge detection\n",
    "2. 池化层用于缓解卷积层对图片过度敏感或者位置平移(例如某一个物体在图片中的位置发生了变化，卷积层就会识别成完全不同的东西)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 卷积层/池化层的输出shape = $int((input\\_shape - kernel\\_shape + padding\\_shape + strides\\_shape)/strides\\_shape)$; padding为same的时候, padding_shape=kernel_shape-1\n",
    "2. 卷积层在channels的shape必须与输入在channels的shape一致，它采取了channel层级相同的输入层和卷积层进行卷积计算，全部channel计算完后，在元素上进行相加，最后输出一个channel的结果。\n",
    "3. 卷积层有多个filters，则输出有多个channels。若只有一个卷积，则只输出一个channels。\n",
    "4. 池化层会对所有的channels做一次池化。即每一层级的channels都会和池化层做一次计算，然后输出。输入有多少个channels，输出就有多少个channels。\n",
    "5. 在tf中，池化的strides默认与pool_size一致。(如tf.keras.layers.MaxPool2D等)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
