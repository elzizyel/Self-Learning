{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正向传播、反向传播和计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面几节里我们使用了小批量随机梯度下降的优化算法来训练模型。在实现中，我们只提供了模型的正向传播（forward propagation）的计算，即对输入计算模型输出，然后通过`autograd`模块来调用系统自动生成的`backward`函数计算梯度。基于反向传播（back-propagation）算法的自动求梯度极大简化了深度学习模型训练算法的实现。本节我们将使用数学和计算图（computational graph）两个方式来描述正向传播和反向传播。具体来说，我们将以带$L_2$范数正则化的含单隐藏层的多层感知机为样例模型解释正向传播和反向传播。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正向传播是指对神经网络沿着从输入层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）。为简单起见，假设输入是一个特征为$\\boldsymbol{x} \\in \\mathbb{R}^d$的样本，且不考虑偏差项，那么中间变量\n",
    "\n",
    "$$\\boldsymbol{z} = \\boldsymbol{W}^{(1)} \\boldsymbol{x},$$\n",
    "\n",
    "其中$\\boldsymbol{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$是隐藏层的权重参数。把中间变量$\\boldsymbol{z} \\in \\mathbb{R}^h$输入按元素运算的激活函数$\\phi$后，将得到向量长度为$h$的隐藏层变量\n",
    "\n",
    "$$\\boldsymbol{h} = \\phi (\\boldsymbol{z}).$$\n",
    "\n",
    "隐藏层变量$\\boldsymbol{h}$也是一个中间变量。假设输出层参数只有权重$\\boldsymbol{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$，可以得到向量长度为$q$的输出层变量\n",
    "\n",
    "$$\\boldsymbol{o} = \\boldsymbol{W}^{(2)} \\boldsymbol{h}.$$\n",
    "\n",
    "假设损失函数为$\\ell$，且样本标签为$y$，可以计算出单个数据样本的损失项\n",
    "\n",
    "$$L = \\ell(\\boldsymbol{o}, y).$$\n",
    "\n",
    "根据$L_2$范数正则化的定义，给定超参数$\\lambda$，正则化项即\n",
    "\n",
    "$$s = \\frac{\\lambda}{2} \\left(\\|\\boldsymbol{W}^{(1)}\\|_F^2 + \\|\\boldsymbol{W}^{(2)}\\|_F^2\\right),$$\n",
    "\n",
    "其中矩阵的Frobenius范数等价于将矩阵变平为向量后计算$L_2$范数。最终，模型在给定的数据样本上带正则化的损失为\n",
    "\n",
    "$$J = L + s.$$\n",
    "\n",
    "我们将$J$称为有关给定数据样本的目标函数，并在以下的讨论中简称目标函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通常绘制计算图来可视化运算符和变量在计算中的依赖关系。下图绘制了本节中样例模型正向传播的计算图，其中左下角是输入，右上角是输出。可以看到，图中箭头方向大多是向右和向上，其中方框代表变量，圆圈代表运算符，箭头表示从输入到输出之间的依赖关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/class4_1.2.svg\" style=\"zoom:100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播指的是计算神经网络参数梯度的方法。总的来说，反向传播依据微积分中的链式法则，沿着从输出层到输入层的顺序，依次计算并存储目标函数有关神经网络各层的中间变量以及参数的梯度。对输入或输出$\\mathsf{X}, \\mathsf{Y}, \\mathsf{Z}$为任意形状张量的函数$\\mathsf{Y}=f(\\mathsf{X})$和$\\mathsf{Z}=g(\\mathsf{Y})$，通过链式法则，我们有\n",
    "\n",
    "$$\\frac{\\partial \\mathsf{Z}}{\\partial \\mathsf{X}} = \\text{prod}\\left(\\frac{\\partial \\mathsf{Z}}{\\partial \\mathsf{Y}}, \\frac{\\partial \\mathsf{Y}}{\\partial \\mathsf{X}}\\right),$$\n",
    "\n",
    "其中$\\text{prod}$运算符将根据两个输入的形状，在必要的操作（如转置和互换输入位置）后对两个输入做乘法。\n",
    "\n",
    "回顾一下本节中样例模型，它的参数是$\\boldsymbol{W}^{(1)}$和$\\boldsymbol{W}^{(2)}$，因此反向传播的目标是计算$\\partial J/\\partial \\boldsymbol{W}^{(1)}$和$\\partial J/\\partial \\boldsymbol{W}^{(2)}$。我们将应用链式法则依次计算各中间变量和参数的梯度，其计算次序与前向传播中相应中间变量的计算次序恰恰相反。首先，分别计算目标函数$J=L+s$有关损失项$L$和正则项$s$的梯度\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial L} = 1, \\quad \\frac{\\partial J}{\\partial s} = 1.$$\n",
    "\n",
    "其次，依据链式法则计算目标函数有关输出层变量的梯度$\\partial J/\\partial \\boldsymbol{o} \\in \\mathbb{R}^q$：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{o}}\n",
    "= \\text{prod}\\left(\\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial \\boldsymbol{o}}\\right)\n",
    "= \\frac{\\partial L}{\\partial \\boldsymbol{o}}.\n",
    "$$\n",
    "\n",
    "\n",
    "接下来，计算正则项有关两个参数的梯度：\n",
    "\n",
    "$$\\frac{\\partial s}{\\partial \\boldsymbol{W}^{(1)}} = \\lambda \\boldsymbol{W}^{(1)},\\quad\\frac{\\partial s}{\\partial \\boldsymbol{W}^{(2)}} = \\lambda \\boldsymbol{W}^{(2)}.$$\n",
    "\n",
    "\n",
    "现在，我们可以计算最靠近输出层的模型参数的梯度$\\partial J/\\partial \\boldsymbol{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$。依据链式法则，得到\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{W}^{(2)}}\n",
    "= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\boldsymbol{o}}, \\frac{\\partial \\boldsymbol{o}}{\\partial \\boldsymbol{W}^{(2)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\boldsymbol{W}^{(2)}}\\right)\n",
    "= \\frac{\\partial J}{\\partial \\boldsymbol{o}} \\boldsymbol{h}^\\top + \\lambda \\boldsymbol{W}^{(2)}.\n",
    "$$\n",
    "\n",
    "\n",
    "沿着输出层向隐藏层继续反向传播，隐藏层变量的梯度$\\partial J/\\partial \\boldsymbol{h} \\in \\mathbb{R}^h$可以这样计算：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{h}}\n",
    "= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\boldsymbol{o}}, \\frac{\\partial \\boldsymbol{o}}{\\partial \\boldsymbol{h}}\\right)\n",
    "= {\\boldsymbol{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\boldsymbol{o}}.\n",
    "$$\n",
    "\n",
    "\n",
    "由于激活函数$\\phi$是按元素运算的，中间变量$\\boldsymbol{z}$的梯度$\\partial J/\\partial \\boldsymbol{z} \\in \\mathbb{R}^h$的计算需要使用按元素乘法符$\\odot$：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{z}}\n",
    "= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\boldsymbol{h}}, \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}\\right)\n",
    "= \\frac{\\partial J}{\\partial \\boldsymbol{h}} \\odot \\phi'\\left(\\boldsymbol{z}\\right).\n",
    "$$\n",
    "\n",
    "最终，我们可以得到最靠近输入层的模型参数的梯度$\\partial J/\\partial \\boldsymbol{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$。依据链式法则，得到\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{W}^{(1)}}\n",
    "= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\boldsymbol{z}}, \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}^{(1)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\boldsymbol{W}^{(1)}}\\right)\n",
    "= \\frac{\\partial J}{\\partial \\boldsymbol{z}} \\boldsymbol{x}^\\top + \\lambda \\boldsymbol{W}^{(1)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练深度学习模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练深度学习模型时，正向传播和反向传播之间相互依赖。下面我们仍然以本节中的样例模型分别阐述它们之间的依赖关系。\n",
    "\n",
    "一方面，正向传播的计算可能依赖于模型参数的当前值，而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的。例如，计算正则化项$s = (\\lambda/2) \\left(\\|\\boldsymbol{W}^{(1)}\\|_F^2 + \\|\\boldsymbol{W}^{(2)}\\|_F^2\\right)$依赖模型参数$\\boldsymbol{W}^{(1)}$和$\\boldsymbol{W}^{(2)}$的当前值，而这些当前值是优化算法最近一次根据反向传播算出梯度后迭代得到的。\n",
    "\n",
    "另一方面，反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的。举例来说，参数梯度$\\partial J/\\partial \\boldsymbol{W}^{(2)} = (\\partial J / \\partial \\boldsymbol{o}) \\boldsymbol{h}^\\top + \\lambda \\boldsymbol{W}^{(2)}$的计算需要依赖隐藏层变量的当前值$\\boldsymbol{h}$。这个当前值是通过从输入层到输出层的正向传播计算并存储得到的。\n",
    "\n",
    "因此，在模型参数初始化完成后，我们交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。既然我们在反向传播中使用了正向传播中计算得到的中间变量来避免重复计算，那么这个复用也导致正向传播结束后不能立即释放中间变量内存。这也是训练要比预测占用更多内存的一个重要原因。另外需要指出的是，这些中间变量的个数大体上与网络层数线性相关，每个变量的大小跟批量大小和输入个数也是线性相关的，它们是导致较深的神经网络使用较大批量训练时更容易超内存的主要原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数值稳定性和模型初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 衰减和爆炸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当神经网络的层数较多时，模型的数值稳定性容易变差。假设一个层数为$L$的多层感知机的第$l$层$\\boldsymbol{H}^{(l)}$的权重参数为$\\boldsymbol{W}^{(l)}$，输出层$\\boldsymbol{H}^{(L)}$的权重参数为$\\boldsymbol{W}^{(L)}$。为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity mapping）$\\phi(x) = x$。给定输入$\\boldsymbol{X}$，多层感知机的第$l$层的输出$\\boldsymbol{H}^{(l)} = \\boldsymbol{X} \\boldsymbol{W}^{(1)} \\boldsymbol{W}^{(2)} \\ldots \\boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\\boldsymbol{X}$分别与$0.2^{30} \\approx 1 \\times 10^{-21}$（衰减）和$5^{30} \\approx 9 \\times 10^{20}$（爆炸）的乘积。类似地，当层数较多时，梯度的计算也更容易出现衰减或爆炸。\n",
    "\n",
    "随着内容的不断深入，我们会在后面的章节进一步介绍深度学习的数值稳定性问题以及解决方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机初始化模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在神经网络中，通常需要随机初始化模型参数。下面我们来解释这样做的原因。\n",
    "\n",
    "回顾多层感知机。为了方便解释，假设输出层只保留一个输出单元 $o_1$（删去$o_2$和$o_3$以及指向它们的箭头），且隐藏层使用相同的激活函数。如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF2.0的默认随机初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机初始化模型参数的方法有很多。在3.3节（线性回归的简洁实现）中，我们使用`kernel_initializer=init.RandomNormal(stddev=0.01)`使模型`model`的权重参数采用正态分布的随机初始化方式。不过，Tensorflow中`initializers`的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考[源代码](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers)），因此一般不用我们考虑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier随机初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一种比较常用的随机初始化方法叫作Xavier随机初始化。\n",
    "假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布\n",
    "$\\left(-\\sqrt{\\frac{6}{a+b}}, \\sqrt{\\frac{6}{a+b}}\\right)$\n",
    "它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T08:31:43.061853Z",
     "start_time": "2020-03-17T08:31:43.057049Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import initializers as init\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T08:31:43.316254Z",
     "start_time": "2020-03-17T08:31:43.262545Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "\n",
    "train_data = pd.read_csv('./data/kaggle_house/train.csv')\n",
    "test_data = pd.read_csv('./data/kaggle_house/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T08:31:43.687526Z",
     "start_time": "2020-03-17T08:31:43.519686Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "\n",
    "all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))\n",
    "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
    "all_features[numeric_features] = all_features[numeric_features].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std()))\n",
    "\n",
    "# 标准化后，每个特征的均值变为0，所以可以直接用0来替换缺失值\n",
    "all_features = all_features.fillna(0)\n",
    "\n",
    "# one_hot encoder\n",
    "all_features = pd.get_dummies(all_features, dummy_na=True)\n",
    "\n",
    "# 转化\n",
    "n_train = train_data.shape[0]\n",
    "train_features = np.array(all_features[:n_train].values,dtype=np.float)\n",
    "test_features = np.array(all_features[n_train:].values,dtype=np.float)\n",
    "train_labels = np.array(train_data.SalePrice.values.reshape(-1, 1),dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T08:36:36.789158Z",
     "start_time": "2020-03-17T08:36:36.777199Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "\n",
    "def get_net():\n",
    "    net = keras.models.Sequential()\n",
    "    net.add(keras.layers.Dense(1))\n",
    "    return net\n",
    "\n",
    "# 定义损失函数\n",
    "log_rmse = tf.keras.losses.mean_squared_logarithmic_error\n",
    "\n",
    "# K-fold 验证\n",
    "def get_k_fold_data(k, i, X, y):\n",
    "    assert k > 1\n",
    "    fold_size = X.shape[0] // k\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = tf.concat([X_train, X_part], axis=0)\n",
    "            y_train = tf.concat([y_train, y_part], axis=0)\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "def k_fold(k, X_train, y_train, num_epochs,\n",
    "           learning_rate, weight_decay, batch_size):\n",
    "    train_l_sum, valid_l_sum = 0, 0\n",
    "    for i in range(k):\n",
    "        # create model\n",
    "        data = get_k_fold_data(k, i, X_train, y_train)\n",
    "        net = get_net()\n",
    "        # Compile model\n",
    "        net.compile(loss=tf.keras.losses.mean_squared_logarithmic_error, optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "        # Fit the model\n",
    "        history=net.fit(data[0], data[1],validation_data=(data[2], data[3]), epochs=num_epochs, batch_size=batch_size,validation_freq=1,verbose=0)\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        print('fold %d, train rmse %f, valid rmse %f'\n",
    "              % (i, loss[-1], val_loss[-1]))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(loss, label='train')\n",
    "    plt.plot(val_loss, label='valid')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T08:46:48.360124Z",
     "start_time": "2020-03-17T08:45:05.914276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0, train rmse 3.222833, valid rmse 3.271283\n",
      "fold 1, train rmse 2.867598, valid rmse 2.909965\n",
      "fold 2, train rmse 2.893340, valid rmse 2.960471\n",
      "fold 3, train rmse 3.149005, valid rmse 3.021711\n",
      "fold 4, train rmse 2.980161, valid rmse 3.009936\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 模型选择\n",
    "# 通常使用K-Fold来选择模型并调节超参数\n",
    "# 或者可以使用一些调参包\n",
    "\n",
    "k, num_epoch, lr, weight_decay, batch_size = 5, 1000, 0.1, 0.2, 64\n",
    "k_fold(k, train_features, train_labels, num_epoch,lr, weight_decay, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "\n",
    "x_train=tf.convert_to_tensor(train_features,dtype=tf.float32)\n",
    "y_train=tf.convert_to_tensor(train_labels,dtype=tf.float32)\n",
    "x_test=tf.convert_to_tensor(test_features,dtype=tf.float32)\n",
    "model=tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "adam=tf.keras.optimizers.Adam(0.5)\n",
    "model.compile(optimizer=adam,\n",
    "              loss=tf.keras.losses.mean_squared_logarithmic_error\n",
    "              )\n",
    "model.fit(x_train, y_train, epochs=200,batch_size=32,verbose=0)\n",
    "preds=np.array(model.predict(x_test))\n",
    "test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n",
    "submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
