<center><font size = 10>SVM</font></center>

# 1. 基础介绍

支持向量机(Support Vector Machine)是一种二分类模型，它的基本模型是定义特征空间上的**间隔最大的线性分类器**。加上了kernel之后，可以使他成为非线性分类器。



SVM的基本想法就是求解能够正确划分训练集数据且几何间隔最大的分离超平面，如下图所示，$wx + b = 0$即为分离超平面，对于线性可分的数据而言，这样的超平面有无穷多个，但是集合间隔最大的超平面是唯一的。即该平面到两侧最近的数据点的距离最大。

<img src="img/svm_1.jpg" style="zoom:30%">



# 2. 公式推导

训练数据集: $T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$, 其中$x_i \in \R^n$，$y_i \in \{+1, -1\}$。$y=+1$时为正例，$y=-1$时为负例。再假设训练数据集是线性可分的。

所以数据集上的点到超平面的距离为 $\gamma_i = y_i(\frac{w}{||w||}·x_i + \frac{b}{||w||})$, 那么所有样本点到超平面的集合最小距离为$\gamma = \min_{i=1,2,...,N} \gamma_i$

所以可以把问题转成约束优化问题

- $\max_{w,b} \gamma$
- $s.t. \ \ \ y_i(\frac{w}{||w||}·x_i + \frac{b}{||w||}) \ge \gamma$ ， $\forall i = 1,2,...,N$

对限制条件两边同除以$\gamma$并且令$w = \frac{w}{||w||\gamma}$ , $b = \frac{b}{||w||\gamma}$

得到 $y_i(w·x_i + b) \ge 1$

又因为最大化$\gamma$，等价于最大化$\frac{1}{||w||}$（因为前面已经让$w = \frac{w}{||w||\gamma}$, 转变一下就是$\gamma = \frac{1}{||w||}$), 等价于最小化$\frac{1}{2}||w||^2$



因此，问题可以转化为

- $\min_{w, b} \frac{1}{2} ||w||^2$
- $s.t. \ \ \  y_i(w·x_i + b) \ge 1$,  $\forall i = 1, 2, ..., N$

之后就是使用拉格朗日乘子定理 + KKT条件来求解该公式了，具体就不论述了。(太难了)























