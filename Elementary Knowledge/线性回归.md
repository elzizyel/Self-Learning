<center><font size = 10>Linear Regression</font></center>

# 1. 基础介绍

多元线性回归公式:
$$
y = \begin{bmatrix}
y_1 \\ y_2 \\...\\y_n
\end{bmatrix},

X = \begin{bmatrix}
1 \ \  x_{11} \ \ ... \ \ x_{1p} \\ 
1 \ \  x_{21} \ \ ... \ \ x_{2p} \\ 
... \ \  ... \ \ ... \ \ ... \\ 
1 \ \  x_{n1} \ \ ... \ \ x_{np} \\ 
\end{bmatrix},

\epsilon = \begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\...\\ \epsilon_n
\end{bmatrix},

\beta = \begin{bmatrix}
\beta_0 \\ \beta_1 \\...\\ \beta_p
\end{bmatrix}
$$

$$
y = X \beta + \epsilon, \ \ \epsilon \in N(0, \sigma^2)
$$

损失函数(MSE):
$$
L = \frac{1}{n} \sum_{i=1}^n (\hat{y_i} - y_i)^2
$$
求解方式:

1. 最小二乘法(Ordinary Least Squares)	
  
   狭义的最小二乘: 指线性回归采用最小二乘准则(即 $(X \beta - y)^2$最小)， 进行线性拟合参数求解的、矩阵形式的公式方法。是线性假设下的一种有闭式解的参数求解方法，最终结果为全局最优。
   $$
   \frac{\partial L}{\partial \beta_0} = -2 \sum_{i=1}^k(\gamma_i - \beta_0 - \beta_1x_{i1} - ... - \beta_p x_{ip}) = 0 \\ 
   \frac{\partial L}{\partial \beta_j} = -2 \sum_{i=1}^k(\gamma_i - \beta_0 - \beta_1x_{i1} - ... - \beta_p x_{ip})x_{ij} = 0, \ \ \forall j \in \{1, p\}
   $$
   上式写成矩阵形式即 $X^TX\beta = X^TY$, 化简得$\hat{\beta} = (X^TX)^{-1}X^TY$

​		广义的最小二乘: 即最小二乘准则(即 $(X \beta - y)^2$最小)。本质上是一种objective function。



2. 梯度下降(Gradient Descent)

   其实就是对$\beta_j$求偏导，求出来的值乘上学习率，与原先的$\beta_j$相减，得到新的$\beta_j$

$$
\beta_j \leftarrow \beta_j - \alpha \frac{\partial L}{\partial \beta_j} = \beta_j - \alpha * \sum_{i=0}^n(y_i - \hat{y}_i)x_{ij}
$$



# 2. 代码

```python
import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Generate Data
x1 = []
x2 = []
y = []
for i in range(10000):
    x1.append(random.uniform(-1000, 1000))
    x2.append(random.uniform(0, 10000))
    y.append(3 * x1[-1] - 2 * x2[-1] + 5)
data = pd.DataFrame()
data['target'] = y
data['x1'] = x1
data['x2'] = x2

# Train Model
y = data['target']
X = data.iloc[:, 1:]
model = LinearRegression()
model.fit(X, y)
print(model.intercept_)
print(model.coef_)
```














