{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Env\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = pd.read_csv('df_final.csv', dtype={'plate_city_code':str,\n",
    "                                            'warehouse_type':int,\n",
    "                                            'warehouse_province_code':str,\n",
    "                                            'warehouse_city_code':str,\n",
    "                                            'warehouse_district_code':str,\n",
    "                                            'shop_province_code':str,\n",
    "                                            'shop_city_code':str,\n",
    "                                            'shop_district_code':str,\n",
    "                                            'is_same_province':int,\n",
    "                                            'is_same_city':int,\n",
    "                                            'is_same_district':int,\n",
    "                                            'if_gps':int,\n",
    "                                            'if_pay':int,\n",
    "                                            'match_hour':int,\n",
    "                                            'label':float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_map1 = data[['warehouse_province_code', 'warehouse_province_name']].rename(columns={'warehouse_province_code':'province_code',\n",
    "                                                                                         'warehouse_province_name':'province_name'}).copy()\n",
    "pro_map2 = data[['shop_province_code', 'shop_province_name']].rename(columns={'shop_province_code':'province_code',\n",
    "                                                                                'shop_province_name':'province_name'}).copy()\n",
    "pro_map = pd.concat([pro_map1, pro_map2]).drop_duplicates().sort_values(by='province_code').reset_index().drop('index', axis=1).copy()\n",
    "pro_map['province_map_code'] = np.arange(len(pro_map))\n",
    "\n",
    "city_map1 = data[['warehouse_city_code', 'warehouse_city_name']].rename(columns={'warehouse_city_code':'city_code',\n",
    "                                                                                 'warehouse_city_name':'city_name'}).copy()\n",
    "city_map2 = data[['shop_city_code', 'shop_city_name']].rename(columns={'shop_city_code':'city_code',\n",
    "                                                                       'shop_city_name':'city_name'}).copy()\n",
    "city_map3 = data[['plate_city_code', 'plate_city_name']].rename(columns={'plate_city_code':'city_code',\n",
    "                                                                         'plate_city_name':'city_name'}).copy()\n",
    "city_map = pd.concat([city_map1, city_map2, city_map3]).drop_duplicates().sort_values(by='city_code').reset_index().drop('index', axis=1).copy()\n",
    "city_map['city_map_code'] = np.arange(len(city_map))\n",
    "\n",
    "df = data.merge(city_map, how='left', left_on='plate_city_code', right_on='city_code').rename(columns={'city_map_code':'plate_city'})\\\n",
    ".merge(pro_map, how='left', left_on='warehouse_province_code', right_on='province_code').rename(columns={'province_map_code':'warehouse_province'})\\\n",
    ".merge(city_map, how='left', left_on='warehouse_city_code', right_on='city_code').rename(columns={'city_map_code':'warehouse_city'})\\\n",
    ".merge(pro_map, how='left', left_on='shop_province_code', right_on='province_code').rename(columns={'province_map_code':'shop_province'})\\\n",
    ".merge(city_map, how='left', left_on='shop_city_code', right_on='city_code').rename(columns={'city_map_code':'shop_city'}).copy()\n",
    "\n",
    "df = df[['plate_city','warehouse_province', 'warehouse_city', 'warehouse_type', \n",
    "         'shop_province', 'shop_city', 'if_gps', 'if_pay', 'match_hour',\n",
    "         'distance', 'is_same_province', 'is_same_city', 'label']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手动调出来的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select needed data\n",
    "\n",
    "X = df.iloc[:, [0, 1,  2,  3,  4,  5,  6,  7,  9, 10, 11]].copy()\n",
    "Y = df.iloc[:, 12].copy()\n",
    "\n",
    "# Set categorical features\n",
    "\n",
    "cat_feats_ind = [c for c,col in enumerate(X.columns) if col != 'distance']\n",
    "\n",
    "# Split data into train, valid and test\n",
    "\n",
    "X_train,X_valid,Y_train,Y_valid = train_test_split(X, Y, test_size=0.1, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 2.65035\tvalid_1's rmse: 2.66994\n",
      "[200]\ttraining's rmse: 2.60621\tvalid_1's rmse: 2.65507\n",
      "[300]\ttraining's rmse: 2.58259\tvalid_1's rmse: 2.6487\n",
      "[400]\ttraining's rmse: 2.56515\tvalid_1's rmse: 2.64547\n",
      "[500]\ttraining's rmse: 2.55086\tvalid_1's rmse: 2.64172\n",
      "[600]\ttraining's rmse: 2.53995\tvalid_1's rmse: 2.64206\n",
      "[700]\ttraining's rmse: 2.53104\tvalid_1's rmse: 2.64042\n",
      "[800]\ttraining's rmse: 2.52319\tvalid_1's rmse: 2.64002\n",
      "[900]\ttraining's rmse: 2.51536\tvalid_1's rmse: 2.64028\n",
      "[1000]\ttraining's rmse: 2.50943\tvalid_1's rmse: 2.64101\n",
      "[1100]\ttraining's rmse: 2.50392\tvalid_1's rmse: 2.64228\n",
      "Early stopping, best iteration is:\n",
      "[816]\ttraining's rmse: 2.52169\tvalid_1's rmse: 2.63921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.7, bagging_freq=1, boosting_type='gbdt',\n",
       "              cat_smooth=32, class_weight=None, colsample_bytree=1.0,\n",
       "              device='gpu', feature_fraction=0.6, gpu_device_id=1,\n",
       "              gpu_platform_id=0, importance_type='split', is_unbalance='false',\n",
       "              lambda_l1=1, lambda_l2=0.8, learning_rate=0.048, max_bin=255,\n",
       "              max_depth=-1, metric='rmse', min_child_samples=20,\n",
       "              min_child_weight=0.001, min_data_in_leaf=20,\n",
       "              min_data_per_group=838, min_split_gain=0.0,\n",
       "              min_sum_hessian_in_leaf=0.001, n_estimators=100000, n_jobs=12,\n",
       "              num_leaves=48, objective=None, random_state=None, reg_alpha=0.0, ...)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hand-tuned | Random with Grid\n",
    "# 因为GPU最多支持bin256，所以这里用cpu训练\n",
    "\n",
    "params = {\n",
    "    'boosting_type':'gbdt',               # boosting方式\n",
    "    'metric':'rmse',                      # 验证模型方式\n",
    "    'is_unbalance':'false',               # 数据是否有偏 | 仅分类时需决定是否开启\n",
    "\n",
    "    # Leaf-wise Tree\n",
    "    'max_depth':-1,                       # 树的最大深度 | 为-1时解除限制\n",
    "    'num_leaves':48,                      # 每颗树上的叶子数量\n",
    "\n",
    "    # For better accuracy\n",
    "    'max_bin':255,                        # 箱内最大特征数\n",
    "    'learning_rate':0.048,                 # 学习率\n",
    "    'n_estimators':100000,                   # 迭代次数\n",
    "\n",
    "    # Solving over-fitting\n",
    "    'min_data_in_leaf':20,                # 一个叶子上的最小数据量\n",
    "    'min_sum_hessian_in_leaf':1e-3,       # 一个叶子上最小的hessian和 \n",
    "    'bagging_fraction':0.7,                 # 做bagging时取的数据百分比\n",
    "    'bagging_freq':1,                     # 做bagging的频率。每 `参数值` 次迭代做一次bagging。为0时禁用bagging\n",
    "    'feature_fraction':0.6,               # 每次迭代中随机选择 `参数值` 百分比的特征用于训练\n",
    "    'lambda_l1':1,                      # L1正则\n",
    "    'lambda_l2':0.8,                      # L2正则\n",
    "    'min_data_per_group':838,             # 每个分类组的最小数据量 | 处理data少或者类别很多的情况\n",
    "    'cat_smooth':32,                     # 用于分类变量 | 减少噪音在分类特征中的影响，尤其对分类后类别里面数据量很少的  \n",
    "    \n",
    "    # Device settings\n",
    "    'device':'gpu',                       # 训练的设备 | 'cpu'/'gpu'\n",
    "    'gpu_platform_id':0,                  # GPU训练的平台               \n",
    "    'gpu_device_id': 1,                   # GPU训练的设备号\n",
    "    'n_jobs':12\n",
    "}\n",
    "\n",
    "model = lgb.LGBMRegressor(\n",
    "    boosting_type=params['boosting_type'],\n",
    "    metric=params['metric'],\n",
    "    is_unbalance=params['is_unbalance'],\n",
    "    max_depth=params['max_depth'],\n",
    "    num_leaves=params['num_leaves'],\n",
    "    max_bin=params['max_bin'],\n",
    "    learning_rate=params['learning_rate'],\n",
    "    n_estimators=params['n_estimators'],\n",
    "    min_data_in_leaf=params['min_data_in_leaf'],\n",
    "    min_sum_hessian_in_leaf=params['min_sum_hessian_in_leaf'],\n",
    "    bagging_fraction=params['bagging_fraction'],\n",
    "    bagging_freq=params['bagging_freq'],\n",
    "    feature_fraction=params['feature_fraction'],\n",
    "    lambda_l1=params['lambda_l1'],\n",
    "    lambda_l2=params['lambda_l2'],\n",
    "    min_data_per_group=params['min_data_per_group'],\n",
    "    cat_smooth=params['cat_smooth'],\n",
    "    device=params['device'],\n",
    "    gpu_platform_id=params['gpu_platform_id'],\n",
    "    gpu_device_id=params['gpu_device_id'],\n",
    "    n_jobs = params['n_jobs']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, Y_train,\n",
    "    eval_set=[(X_train, Y_train),(X_valid, Y_valid)], # 使用两个数据进行评估\n",
    "    eval_metric='rmse',\n",
    "    categorical_feature=cat_feats_ind,\n",
    "    early_stopping_rounds=300,\n",
    "    verbose=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPE - hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = lgb.Dataset(X_train, Y_train, free_raw_data=False)\n",
    "valid = lgb.Dataset(X_valid, Y_valid, reference=train, free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当传入参数需为整数时，使用hp.quniform(label, low, high, step); 并且需要在训练参数里加上int()\n",
    "# 当传入参数为实数时，使用hp.uniform(label, low, high)\n",
    "# 当传入参数为类别时，使用hp.choice(label, [可选值])\n",
    "    # e.g. 'boosting_type': hp.choice('boosting_type', \n",
    "    #                                 [{'boosting_type': 'gbdt', \n",
    "    #                                   'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n",
    "    #                                  {'boosting_type': 'dart', \n",
    "    #                                   'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
    "    #                                  {'boosting_type': 'goss'}\n",
    "    #                                 ])\n",
    "    # 即当选择boosting_type为gbdt的时候，连带的我需要选择subsample的参数值\n",
    "    # 当选择boosting_type为goss，因为无subsample这个参数，所以不需要选\n",
    "\n",
    "space = {\n",
    "    'num_leaves':hp.quniform('num_leaves', 30, 151, 1),\n",
    "    'max_bin':hp.quniform('max_bin', 32, 256, 1),\n",
    "    # 'learning_rate':hp.uniform('learning_rate', 0.001, 0.1),\n",
    "    'feature_fraction':hp.uniform('feature_fraction', 0.4, 1),\n",
    "    'bagging_fraction':hp.uniform('bagging_fraction', 0.4, 1),\n",
    "    'bagging_freq':hp.quniform('bagging_freq', 0, 100, 1),\n",
    "    'lambda_l1':hp.uniform('lambda_l1', 0, 5),\n",
    "    'lambda_l2':hp.uniform('lambda_l2', 0, 5),\n",
    "    'min_data_per_group':hp.quniform('min_data_per_group', 50, 1000, 1),\n",
    "    'cat_smooth':hp.uniform('cat_smooth', 5, 100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    \n",
    "    params = {\n",
    "        'boosting_type':'gbdt',             \n",
    "        'metric':'rmse',                     \n",
    "        'is_unbalance':'false',               \n",
    "\n",
    "        # Leaf-wise Tree\n",
    "        'max_depth':-1,                       \n",
    "        'num_leaves':int(space['num_leaves']),                      \n",
    "\n",
    "        # For better accuracy\n",
    "        'max_bin':int(space['max_bin']),                       \n",
    "        'learning_rate':0.05,                \n",
    "        # 'n_estimators':10000,                 \n",
    "\n",
    "        # Solving over-fitting\n",
    "        'min_data_in_leaf':20,               \n",
    "        'min_sum_hessian_in_leaf':1e-3,      \n",
    "        'bagging_fraction':space['bagging_fraction'],                 \n",
    "        'bagging_freq':int(space['bagging_freq']),                    \n",
    "        'feature_fraction':space['feature_fraction'],              \n",
    "        'lambda_l1':space['lambda_l1'],                        \n",
    "        'lambda_l2':space['lambda_l2'],                        \n",
    "        'min_data_per_group':int(space['min_data_per_group']),             \n",
    "        'cat_smooth':space['cat_smooth'],                   \n",
    "        \n",
    "        # Device settings\n",
    "        'device':'gpu',                      \n",
    "        'gpu_platform_id':0,                      \n",
    "        'gpu_device_id':1,                  \n",
    "        'n_jobs': 12\n",
    "    }\n",
    "    \n",
    "    cv_results = lgb.cv(\n",
    "        params=params, \n",
    "        train_set=train, \n",
    "        num_boost_round=100000, \n",
    "        nfold=3,\n",
    "        stratified=False, # Whether to perform stratified sampling\n",
    "        early_stopping_rounds=100, \n",
    "        categorical_feature=cat_feats_ind,\n",
    "        verbose_eval=None\n",
    "    )\n",
    "    \n",
    "    return min(cv_results['rmse-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [32:34:04<00:00, 23.45s/it, best loss: 2.6812662515756034]   \n"
     ]
    }
   ],
   "source": [
    "# 想办法通过Trails实现early-stop以及途中的画图功能，下次再尝试\n",
    "\n",
    "best = fmin(\n",
    "    objective, \n",
    "    space, \n",
    "    algo=tpe.suggest, \n",
    "    max_evals=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 2.61517\tvalid_1's rmse: 2.65507\n",
      "[200]\ttraining's rmse: 2.56058\tvalid_1's rmse: 2.64473\n",
      "[300]\ttraining's rmse: 2.53353\tvalid_1's rmse: 2.64477\n",
      "[400]\ttraining's rmse: 2.51553\tvalid_1's rmse: 2.64764\n",
      "[500]\ttraining's rmse: 2.50134\tvalid_1's rmse: 2.65154\n",
      "Early stopping, best iteration is:\n",
      "[270]\ttraining's rmse: 2.5402\tvalid_1's rmse: 2.64371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.98, bagging_freq=9, boosting_type='gbdt',\n",
       "              cat_smooth=85.2, class_weight=None, colsample_bytree=1.0,\n",
       "              device='gpu', feature_fraction=0.7, gpu_device_id=1,\n",
       "              gpu_platform_id=0, importance_type='split', is_unbalance='false',\n",
       "              lambda_l1=4.8, lambda_l2=3.7, learning_rate=0.05, max_bin=74,\n",
       "              max_depth=-1, metric='rmse', min_child_samples=20,\n",
       "              min_child_weight=0.001, min_data_in_leaf=20,\n",
       "              min_data_per_group=273, min_split_gain=0.0,\n",
       "              min_sum_hessian_in_leaf=0.001, n_estimators=10000, n_jobs=12,\n",
       "              num_leaves=143, objective=None, random_state=None, reg_alpha=0.0, ...)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type':'gbdt',             \n",
    "    'metric':'rmse',                     \n",
    "    'is_unbalance':'false',               \n",
    "\n",
    "    # Leaf-wise Tree\n",
    "    'max_depth':-1,                       \n",
    "    'num_leaves':143,                      \n",
    "\n",
    "    # For better accuracy\n",
    "    'max_bin':74,                       \n",
    "    'learning_rate':0.05,                \n",
    "    'n_estimators':10000,                 \n",
    "\n",
    "    # Solving over-fitting\n",
    "    'min_data_in_leaf':20,               \n",
    "    'min_sum_hessian_in_leaf':1e-3,      \n",
    "    'bagging_fraction':0.98,                 \n",
    "    'bagging_freq':9,                    \n",
    "    'feature_fraction':0.7,              \n",
    "    'lambda_l1':4.8,                        \n",
    "    'lambda_l2':3.7,                        \n",
    "    'min_data_per_group':273,             \n",
    "    'cat_smooth':85.2,                   \n",
    "\n",
    "    # Device settings\n",
    "    'device':'gpu',                      \n",
    "    'gpu_platform_id':0,                      \n",
    "    'gpu_device_id':1,                  \n",
    "    'n_jobs': 12\n",
    "}\n",
    "\n",
    "model = lgb.LGBMRegressor(\n",
    "    boosting_type=params['boosting_type'],\n",
    "    metric=params['metric'],\n",
    "    is_unbalance=params['is_unbalance'],\n",
    "    max_depth=params['max_depth'],\n",
    "    num_leaves=params['num_leaves'],\n",
    "    max_bin=params['max_bin'],\n",
    "    learning_rate=params['learning_rate'],\n",
    "    n_estimators=params['n_estimators'],\n",
    "    min_data_in_leaf=params['min_data_in_leaf'],\n",
    "    min_sum_hessian_in_leaf=params['min_sum_hessian_in_leaf'],\n",
    "    bagging_fraction=params['bagging_fraction'],\n",
    "    bagging_freq=params['bagging_freq'],\n",
    "    feature_fraction=params['feature_fraction'],\n",
    "    lambda_l1=params['lambda_l1'],\n",
    "    lambda_l2=params['lambda_l2'],\n",
    "    min_data_per_group=params['min_data_per_group'],\n",
    "    cat_smooth=params['cat_smooth'],\n",
    "    device=params['device'],\n",
    "    gpu_platform_id=params['gpu_platform_id'],\n",
    "    gpu_device_id=params['gpu_device_id'],\n",
    "    n_jobs = params['n_jobs']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, Y_train,\n",
    "    eval_set=[(X_train, Y_train),(X_valid, Y_valid)], # 使用两个数据进行评估\n",
    "    eval_metric='rmse',\n",
    "    categorical_feature=cat_feats_ind,\n",
    "    early_stopping_rounds=300,\n",
    "    verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
