<center><font size = 10>谱聚类</font></center>

## 谱聚类特点

1. 一般而言，`相似矩阵S`选择RBF高斯核函数 --> 求数据点之间的权重。然后使用`NCut`来切图 --> 最小化子图间权重，最大化子图内权重
2. 因为本质上是使用`相似矩阵S`构建出的`邻接矩阵W`来决定点间的权重，所以处理稀疏数据的聚类效果很有效。
3. 因为处理过程中涉及了降维，所以对高维数据进行聚类时的复杂度比传统聚类算法好
4. 如果最终聚类结果的维度`k`很高，有可能因为降维的幅度不够，使得谱聚类的运行速度和聚类效果不好。
5. 谱聚类依赖于相似矩阵S，不同的S选择导致的最终聚类效果不同
6. 仍旧需要选择最终分成多少k类





## 0. 概述

​	谱聚类的主要思想是把所有的数据看成空间中的点，把这些点用边连接到一起。距离远的点边权重低，距离近的边权重高。将所有点用边连接到一起，进行切图，使得切图后子图内边权重尽量高，子图间边权重尽量低，从而达到聚类的目的。







## 1. 无向权重图

​	`图G` = `点集V` + `边集E` 

V = (v~1~, v~2~, ..., v~n~) | V即数据集里的所有点

w~ij~ = v~i~ 与 v~j~ 之间的权重 | 因为为无向权重图 w~ij~ = w~ji~ , 且所有w ≥ 0

$d_i = \sum_{j=1}^{n} w_{ij}$| 对于图中的任意一个点v~i~, 它的度d~i~ 定义为何它相连的所有边的权重之和 

$D = 
\begin{bmatrix}
d_1 &\cdots & \cdots \\
\cdots &d_2 & \cdots \\
\vdots & \vdots & \ddots \\
\cdots & \cdots & d_n
\end {bmatrix}$ | 度矩阵D，它是一个对角矩阵，只有主对角线有值且表示第i行第i列为点v~i~的度数

$W = 
\begin{bmatrix}
w_{11} &    \cdots & w_{1j} \\
w_{21} & \vdots & w_{2j} \\
\vdots & \vdots & \vdots \\
w_{i1} & \cdots & w_{ij}
\end {bmatrix}$ | 邻接矩阵W， 第i行第j列为w~ij~







## 2. 相似矩阵

​	相似矩阵S是用来求邻接矩阵W的。构建邻接矩阵W有三种方法，仅介绍全连接法。

​	全连接法可以选择不同的核函数来定义为边权重，常用的有多项式核函数、高斯核函数、Sigmoid核函数。常用的为RBF高斯核函数，此时的相似矩阵和邻接矩阵相同:


$$
W_{ij} = S_{ij} = exp(- \frac{||x_i - x_j||^2_2}{2\sigma^2}) = exp(\gamma||x_i - x_j||^2_2)
$$

​	

##### 2.1 高斯核函数介绍

​	$RBF = exp(- \frac{||x_i - x_j||^2_2}{2\sigma^2}) = exp(\gamma||x_i - x_j||^2_2)$

​	其中$||x_i-x_j||^2_2$为$x_i$与$x_j$之间的欧氏距离的平方

​	$\sigma$为带宽，用于控制径向作用范围

​	如下图所示，随着$x_i$与$x_j$的距离增加，其RBF函数值单调递减。且$\sigma$越大，RBF的局部影响范围越大。

![sigma=1](/Users/elziz/Self-Learning/Machine-Learning/Spectral-Clustring/sigma=1.png)

![sigma=1](/Users/elziz/Self-Learning/Machine-Learning/Spectral-Clustring/sigma=5.png)



​	其实，核函数是为了解决图在低维空间不可分，映射到高维空间可分所解决的。

​	在这里利用了核函数的特征，来构建相似矩阵S。







## 3. 拉普拉斯矩阵

​	定义拉普拉斯矩阵为L | $L = D - W$

该矩阵的基本性质可以用来求如何切图:

1) 该矩阵为对称矩阵

2) 它的所有特征值都为实数

3) 对于任意向量f都可得 $f^TLf = \frac{1}{2} \sum^{n}_{ij=1} w_{ij}(f_i - f_j)^2$ | 省略推导过程

4) 该矩阵半正定，对应的n个实数特征值都大于等于0。 最小特征值为0

特征值:

A是n阶矩阵，x为n维非零列向量，λ为A的特征值 | $Ax = \lambda x$





## 4. 无向图切图

​	  对图G进行切图，目标及将G(V, E)切成互相没有连接的`k`个子图，每个子图点的集合为: A~1~, A~2~, ..., A~k~

且满足A~i~ ∩ A~j~ = ∅和A~1~ ∪ A~2~ ∪ … ∪ A~k~ = V

​	对于任意两个子图点的集合$A,B \subset V, A\bigcap B = \emptyset$ , 定义A和B之间的切图权重为:

$$
W(A,B) = \sum_{i \in A \ j \in B} w_{ij}
$$

​	对于k各子图点的集合A~1~, A~2~, ..., A~k~， 定义切图cut为 | 其中$\bar{A}_i$为$A_i$的补集
$$
cut(A_1,A_2,...,A_k) = \frac{1}{2} \sum_{i=1}^{k} W(A_i,\bar{A}_i)
$$






## 5. 切图聚类

​	     一般而言有两种切图方法，`RatioCut` 和 `NCut` 。

​	RatioCut 不光考虑最小化$cut(A_1,A_2,...,A_k) $，还考虑最大化子图点的个数

​	NCut 不光考虑最小化$cut(A_1,A_2,...,A_k) $， 还考虑最大化子图点内的权重		

​	一般而言NCut优于RatioCut





## 6. 谱聚类算法流程

1. 选择相似矩阵S的生成方式(默认全连接)
2. 根据相似矩阵S构建邻接矩阵W与度矩阵D
3. 计算拉普拉斯矩阵L
4. 构建标准化后的拉普拉斯矩阵$D^{-\frac{1}{2}} L D^{-\frac{1}{2}}$
5. 计算$D^{-\frac{1}{2}} L D^{-\frac{1}{2}}$最小的k~1~个特征值所各自对应的特征向量$f$
6. 将各自对应的特征向量$f$组成的矩阵按行标准化，形成$n * k_1$维的特征矩阵F
7. 对F中的每一行作为一个k~1~维的样本，共n个样本，用输入的Cut方法进行聚类，形成k~2~类
8. 得类$C(c_1,c_2, ...,c_{k2})$

